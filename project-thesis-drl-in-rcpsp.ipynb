{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\n# DQN Agent\n# -----------------------------\n\n# -----------------------------\n# DDQN Agent\n# -----------------------------\n\n    # Use the same update_epsilon() method as in DQNAgent.\n\n# -----------------------------\n# PPO Agent\n# -----------------------------\n\n\n# -----------------------------\n# RCPSP Environment with Makespan Calculation and Critical Path KPI\n# -----------------------------\n\n\n# -----------------------------\n# Training Loop Helper Function\n# -----------------------------\n\n# -----------------------------\n# Main: Compare DQN, DDQN, PPO, and GPHH Optimization\n# -----------------------------\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:10:03.377850Z","iopub.execute_input":"2025-02-07T20:10:03.378147Z","iopub.status.idle":"2025-02-07T20:10:03.382217Z","shell.execute_reply.started":"2025-02-07T20:10:03.378116Z","shell.execute_reply":"2025-02-07T20:10:03.381287Z"},"editable":false},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Importing Libraries","metadata":{"editable":false}},{"cell_type":"code","source":"!pip install torch-geometric\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport random\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.data import Data\nfrom collections import deque\nimport math\nimport pandas as pd\nimport networkx as nx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:10:03.383467Z","iopub.execute_input":"2025-02-07T20:10:03.383738Z","iopub.status.idle":"2025-02-07T20:10:16.782789Z","shell.execute_reply.started":"2025-02-07T20:10:03.383708Z","shell.execute_reply":"2025-02-07T20:10:16.782088Z"},"editable":false},"outputs":[{"name":"stdout","text":"Collecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.11)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.9.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2025.1.31)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Defining Graph Attention Network","metadata":{"editable":false}},{"cell_type":"code","source":"# -----------------------------\n# Graph Attention Network (GAT)\n# -----------------------------\nclass GATQNetwork(nn.Module):\n    def __init__(self, in_features, hidden_dim, out_features):\n        super(GATQNetwork, self).__init__()\n        self.gat1 = GATConv(in_features, hidden_dim, heads=4, concat=True)\n        self.gat2 = GATConv(hidden_dim * 4, hidden_dim, heads=4, concat=True)\n        self.fc = nn.Linear(hidden_dim * 4, out_features)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.relu(self.gat1(x, edge_index))\n        x = F.relu(self.gat2(x, edge_index))\n        q_values = self.fc(x).mean(dim=0)\n        return q_values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:10:16.784164Z","iopub.execute_input":"2025-02-07T20:10:16.784598Z","iopub.status.idle":"2025-02-07T20:10:16.789851Z","shell.execute_reply.started":"2025-02-07T20:10:16.784572Z","shell.execute_reply":"2025-02-07T20:10:16.789062Z"},"editable":false},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"DQN Agent","metadata":{"editable":false}},{"cell_type":"code","source":"class DQNAgent:\n    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.001, gamma=0.95):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.epsilon = 1.0\n        # Change decay parameters so that epsilon decays only once per episode\n        self.epsilon_decay = 0.995  # decay factor per episode\n        self.epsilon_min = 0.1      # keep a higher minimum epsilon for more exploration\n        self.batch_size = 32\n        self.memory = deque(maxlen=5000)\n\n        self.q_network = GATQNetwork(state_dim, hidden_dim, action_dim).to(self.device)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n\n    def select_action(self, state):\n        if np.random.rand() < self.epsilon:\n            return np.random.choice(self.action_dim)\n        state = state.to(self.device)\n        with torch.no_grad():\n            q_values = self.q_network(state)\n        return torch.argmax(q_values).item()\n\n    def store_experience(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def train(self):\n        if len(self.memory) < self.batch_size:\n            return\n        batch = random.sample(self.memory, self.batch_size)\n        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n        \n        state_batch = [s.to(self.device) for s in state_batch]\n        next_state_batch = [s.to(self.device) for s in next_state_batch]\n        action_batch = torch.tensor(action_batch, dtype=torch.long, device=self.device)\n        reward_batch = torch.tensor(reward_batch, dtype=torch.float32, device=self.device)\n        done_batch = torch.tensor(done_batch, dtype=torch.float32, device=self.device)\n        \n        q_values = torch.stack([self.q_network(s) for s in state_batch])\n        next_q_values = torch.stack([self.q_network(s) for s in next_state_batch]).max(dim=1)[0]\n        \n        target_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)\n        q_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n        \n        loss = F.mse_loss(q_values, target_q_values)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n    def update_epsilon(self):\n        # Update epsilon once per episode\n        if self.epsilon > self.epsilon_min:\n            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:10:16.790870Z","iopub.execute_input":"2025-02-07T20:10:16.791187Z","iopub.status.idle":"2025-02-07T20:10:16.815718Z","shell.execute_reply.started":"2025-02-07T20:10:16.791143Z","shell.execute_reply":"2025-02-07T20:10:16.815056Z"},"editable":false},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"DDQN Agent","metadata":{"editable":false}},{"cell_type":"code","source":"class DDQNAgent(DQNAgent):\n    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.001, gamma=0.95):\n        super(DDQNAgent, self).__init__(state_dim, action_dim, hidden_dim, lr, gamma)\n        self.target_network = GATQNetwork(state_dim, hidden_dim, action_dim).to(self.device)\n        self.update_target_network()\n\n    def update_target_network(self):\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n    def train(self):\n        if len(self.memory) < self.batch_size:\n            return\n        batch = random.sample(self.memory, self.batch_size)\n        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n        \n        state_batch = [s.to(self.device) for s in state_batch]\n        next_state_batch = [s.to(self.device) for s in next_state_batch]\n        action_batch = torch.tensor(action_batch, dtype=torch.long, device=self.device)\n        reward_batch = torch.tensor(reward_batch, dtype=torch.float32, device=self.device)\n        done_batch = torch.tensor(done_batch, dtype=torch.float32, device=self.device)\n        \n        q_values = torch.stack([self.q_network(s) for s in state_batch])\n        next_actions = torch.stack([self.q_network(s) for s in next_state_batch]).argmax(dim=1)\n        next_q_values_target = torch.stack([self.target_network(s) for s in next_state_batch])\n        next_q_values = next_q_values_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n        \n        target_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)\n        q_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n        \n        loss = F.mse_loss(q_values, target_q_values)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:10:16.816464Z","iopub.execute_input":"2025-02-07T20:10:16.816712Z","iopub.status.idle":"2025-02-07T20:10:16.835263Z","shell.execute_reply.started":"2025-02-07T20:10:16.816679Z","shell.execute_reply":"2025-02-07T20:10:16.834618Z"},"editable":false},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"PPO Agent","metadata":{"editable":false}},{"cell_type":"code","source":"class PPOAgent:\n    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.001, gamma=0.95,\n                 clip_epsilon=0.2, update_epochs=10):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.gamma = gamma\n        self.clip_epsilon = clip_epsilon\n        self.update_epochs = update_epochs\n        \n        self.policy = GATQNetwork(state_dim, hidden_dim, action_dim).to(self.device)\n        self.old_policy = GATQNetwork(state_dim, hidden_dim, action_dim).to(self.device)\n        self.old_policy.load_state_dict(self.policy.state_dict())\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n        \n        self.memory = []\n\n    def select_action(self, state):\n        state = state.to(self.device)\n        with torch.no_grad():\n            logits = self.policy(state)\n            probabilities = F.softmax(logits, dim=-1)\n            action = torch.multinomial(probabilities, 1).item()\n        return action\n\n    def store_experience(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def train(self):\n        if not self.memory:\n            return\n        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*self.memory)\n        state_batch = [s.to(self.device) for s in state_batch]\n        action_batch = torch.tensor(action_batch, dtype=torch.long, device=self.device)\n        reward_batch = torch.tensor(reward_batch, dtype=torch.float32, device=self.device)\n        done_batch = torch.tensor(done_batch, dtype=torch.float32, device=self.device)\n        \n        for _ in range(self.update_epochs):\n            old_logits = torch.stack([self.old_policy(s) for s in state_batch])\n            old_probs = F.softmax(old_logits, dim=-1).gather(1, action_batch.unsqueeze(1)).squeeze(1)\n            \n            new_logits = torch.stack([self.policy(s) for s in state_batch])\n            new_probs = F.softmax(new_logits, dim=-1).gather(1, action_batch.unsqueeze(1)).squeeze(1)\n            \n            ratios = new_probs / (old_probs + 1e-8)\n            advantages = reward_batch + self.gamma * (1 - done_batch) - reward_batch\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n            loss = -torch.min(surr1, surr2).mean()\n            \n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n        \n        self.old_policy.load_state_dict(self.policy.state_dict())\n        self.memory = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:10:16.836044Z","iopub.execute_input":"2025-02-07T20:10:16.836318Z","iopub.status.idle":"2025-02-07T20:10:16.854417Z","shell.execute_reply.started":"2025-02-07T20:10:16.836286Z","shell.execute_reply":"2025-02-07T20:10:16.853629Z"},"editable":false},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"RCPSP Enviorment","metadata":{"editable":false}},{"cell_type":"code","source":"class RCPSPEnv:\n    def __init__(self, instance_file=None, num_tasks=20, resource_limits=[10, 10, 10]):\n        self.num_tasks = num_tasks\n        self.resource_limits = resource_limits\n        self.graph = self.generate_graph(num_tasks)\n        self.resource_limits_array = np.array(self.resource_limits, dtype=int)\n        self.horizon = 10000\n        self.resource_usage = pd.DataFrame(\n            np.zeros((self.horizon, len(self.resource_limits)), dtype=int),\n            columns=range(len(self.resource_limits))\n        )\n        self.task_ids = sorted(list(self.graph.nodes))\n        self.reset()\n    \n    def reset(self):\n        self.completed_tasks = set()\n        self.schedule = {}  # Task start times\n        self.time_elapsed = 0\n        self.resource_usage.iloc[:, :] = 0\n        return self.get_state()\n\n    def generate_graph(self, num_tasks):\n        G = nx.DiGraph()\n        for i in range(num_tasks):\n            G.add_node(\n                i,\n                duration=np.random.randint(5, 20),\n                resources=[np.random.randint(1, 5) for _ in self.resource_limits]\n            )\n        for _ in range(num_tasks // 2):\n            a, b = np.random.choice(num_tasks, 2, replace=False)\n            if a != b and not nx.has_path(G, b, a):\n                G.add_edge(a, b)\n        return G\n\n    def get_state(self):\n        node_features = []\n        for i in self.graph.nodes:\n            duration = self.graph.nodes[i]['duration']\n            resources = self.graph.nodes[i]['resources']\n            is_scheduled = 1 if i in self.completed_tasks else 0\n            node_features.append([duration] + resources + [is_scheduled])\n        if self.graph.number_of_edges() > 0:\n            edge_index = torch.tensor(list(self.graph.edges), dtype=torch.long).t().contiguous()\n        else:\n            edge_index = torch.empty((2, 0), dtype=torch.long)\n        node_features = torch.tensor(node_features, dtype=torch.float)\n        return Data(x=node_features, edge_index=edge_index)\n\n    def compute_makespan(self):\n        if not self.completed_tasks:\n            return 0\n        finish_times = [self.schedule[task] + self.graph.nodes[task]['duration'] for task in self.completed_tasks]\n        return max(finish_times) if finish_times else 0\n\n    def compute_critical_path_length(self):\n        cp = {node: 0 for node in self.graph.nodes}\n        for node in nx.topological_sort(self.graph):\n            duration = self.graph.nodes[node]['duration']\n            preds = list(self.graph.predecessors(node))\n            if preds:\n                cp[node] = max(cp[pred] for pred in preds) + duration\n            else:\n                cp[node] = duration\n        return max(cp.values())\n\n    def step(self, action):\n        if hasattr(self, 'task_ids'):\n            if action < 0 or action >= len(self.task_ids):\n                print(f\"Warning: action index {action} out of range; clipping to maximum index {len(self.task_ids)-1}\")\n                action = len(self.task_ids) - 1\n            actual_action = self.task_ids[action]\n        else:\n            actual_action = action\n\n        # Instead of terminating the episode on an invalid action,\n        # we penalize it and continue.\n        if actual_action in self.completed_tasks:\n            return self.get_state(), -10, False\n\n        try:\n            predecessors = list(self.graph.predecessors(actual_action))\n        except Exception as e:\n            print(f\"Error getting predecessors for task {actual_action}: {e}\")\n            predecessors = []\n        start_time = max([self.schedule.get(p, 0) + self.graph.nodes[p]['duration'] for p in predecessors], default=0)\n        task_resources = np.array(self.graph.nodes[actual_action]['resources'], dtype=int)\n        duration = self.graph.nodes[actual_action]['duration']\n        found_feasible = False\n        candidate = start_time\n        limits = self.resource_limits_array\n        while candidate <= self.horizon - duration:\n            usage_slice = self.resource_usage.iloc[candidate:candidate+duration, :]\n            if np.all(usage_slice.values + task_resources <= limits):\n                found_feasible = True\n                break\n            candidate += 1\n        if not found_feasible:\n            candidate = self.horizon - duration\n        start_time = candidate\n        self.resource_usage.iloc[start_time:start_time+duration, :] += task_resources\n        self.schedule[actual_action] = start_time\n        self.completed_tasks.add(actual_action)\n        self.time_elapsed = max(self.time_elapsed, start_time + duration)\n        done = len(self.completed_tasks) == self.num_tasks\n        makespan = self.compute_makespan()\n        reward = -makespan if done else 0\n        return self.get_state(), reward, done\n\n    def schedule_from_ordering(self, ordering):\n        self.completed_tasks = set()\n        self.schedule = {}\n        self.time_elapsed = 0\n        self.resource_usage = pd.DataFrame(\n            np.zeros((self.horizon, len(self.resource_limits)), dtype=int),\n            columns=range(len(self.resource_limits))\n        )\n        for task in ordering:\n            predecessors = list(self.graph.predecessors(task))\n            start_time = max([self.schedule.get(p, 0) + self.graph.nodes[p]['duration'] for p in predecessors], default=0)\n            task_resources = np.array(self.graph.nodes[task]['resources'], dtype=int)\n            duration = self.graph.nodes[task]['duration']\n            found_feasible = False\n            candidate = start_time\n            limits = self.resource_limits_array\n            while candidate <= self.horizon - duration:\n                usage_slice = self.resource_usage.iloc[candidate:candidate+duration, :]\n                if np.all(usage_slice.values + task_resources <= limits):\n                    found_feasible = True\n                    break\n                candidate += 1\n            if not found_feasible:\n                candidate = self.horizon - duration\n            start_time = candidate\n            self.resource_usage.iloc[start_time:start_time+duration, :] += task_resources\n            self.schedule[task] = start_time\n            self.completed_tasks.add(task)\n            self.time_elapsed = max(self.time_elapsed, start_time + duration)\n        return self.compute_makespan()\n\n    # -----------------------------\n    # Genetic Programming Hyper-Heuristic (GPHH) Method\n    # -----------------------------\n    def schedule_from_weights(self, weights):\n        self.completed_tasks = set()\n        self.schedule = {}\n        self.time_elapsed = 0\n        ordering = []\n        unscheduled = set(self.graph.nodes)\n        feasible = [task for task in self.graph.nodes if self.graph.in_degree(task) == 0]\n        while unscheduled:\n            if not feasible:\n                break\n            scores = {}\n            for task in feasible:\n                duration = self.graph.nodes[task]['duration']\n                resources = self.graph.nodes[task]['resources']\n                score = weights[0]*duration + weights[1]*resources[0] + weights[2]*resources[1] + weights[3]*resources[2]\n                scores[task] = score\n            chosen = max(scores, key=scores.get)\n            ordering.append(chosen)\n            unscheduled.remove(chosen)\n            feasible.remove(chosen)\n            for succ in self.graph.successors(chosen):\n                if succ in unscheduled and all(pred in ordering for pred in self.graph.predecessors(succ)):\n                    feasible.append(succ)\n        return self.schedule_from_ordering(ordering)\n\n    def optimize_schedule_gphh(self, population_size=50, generations=100, mutation_prob=0.3):\n        pop = [np.random.uniform(-10, 10, 4).tolist() for _ in range(population_size)]\n        def fitness(weights):\n            return self.schedule_from_weights(weights)\n        best_weights = None\n        best_fit = float('inf')\n        for g in range(generations):\n            fit_values = [fitness(ind) for ind in pop]\n            min_idx = np.argmin(fit_values)\n            if fit_values[min_idx] < best_fit:\n                best_fit = fit_values[min_idx]\n                best_weights = pop[min_idx]\n            sorted_pop = [ind for _, ind in sorted(zip(fit_values, pop), key=lambda pair: pair[0])]\n            survivors = sorted_pop[:population_size//2]\n            new_pop = survivors.copy()\n            while len(new_pop) < population_size:\n                parent = random.choice(survivors)\n                child = parent.copy()\n                for i in range(len(child)):\n                    if random.random() < mutation_prob:\n                        child[i] += np.random.normal(0, 1)\n                        child[i] = max(min(child[i], 10), -10)\n                new_pop.append(child)\n            pop = new_pop\n        return best_weights, best_fit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:10:16.855215Z","iopub.execute_input":"2025-02-07T20:10:16.855471Z","iopub.status.idle":"2025-02-07T20:10:16.879909Z","shell.execute_reply.started":"2025-02-07T20:10:16.855440Z","shell.execute_reply":"2025-02-07T20:10:16.879261Z"},"editable":false},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Training Agents","metadata":{"editable":false}},{"cell_type":"code","source":"def run_agent(agent, env, num_episodes=15, agent_name='Agent'):\n    episode_rewards = []\n    episode_makespans = []\n    episode_deviations = []\n    best_makespan = float('inf')\n    best_policy = None\n    best_episode = -1\n    cp_length = env.compute_critical_path_length()  # Compute the critical path length once\n    for episode in range(num_episodes):\n        state = env.reset()\n        total_reward = 0\n        done = False\n        while not done:\n            action = agent.select_action(state)\n            next_state, reward, done = env.step(action)\n            agent.store_experience(state, action, reward, next_state, done)\n            state = next_state\n            total_reward += reward\n            if not isinstance(agent, PPOAgent):\n                agent.train()\n        if isinstance(agent, PPOAgent):\n            agent.train()\n        if hasattr(agent, 'update_target_network'):\n            agent.update_target_network()\n        makespan = env.compute_makespan()\n        deviation = makespan - cp_length\n        episode_rewards.append(total_reward)\n        episode_makespans.append(makespan)\n        episode_deviations.append(deviation)\n        eps_str = f\"{agent.epsilon:.4f}\" if hasattr(agent, 'epsilon') else \"N/A\"\n        print(f\"{agent_name} Episode {episode+1}/{num_episodes} - Makespan: {makespan}, Deviation: {deviation}, Total Reward: {total_reward}, Epsilon: {eps_str}\")\n        if makespan < best_makespan:\n            best_makespan = makespan\n            best_episode = episode + 1\n            if hasattr(agent, 'q_network'):\n                best_policy = agent.q_network.state_dict().copy()\n            elif hasattr(agent, 'policy'):\n                best_policy = agent.policy.state_dict().copy()\n        # Update epsilon only once per episode\n        if hasattr(agent, 'update_epsilon'):\n            agent.update_epsilon()\n    avg_deviation = np.mean(episode_deviations)\n    print(f\"Best {agent_name} policy found in Episode {best_episode} with Makespan: {best_makespan}\")\n    print(f\"Average deviation from critical path ({cp_length}): {avg_deviation}\")\n    return episode_rewards, episode_makespans, best_policy, avg_deviation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:10:16.880559Z","iopub.execute_input":"2025-02-07T20:10:16.880747Z","iopub.status.idle":"2025-02-07T20:10:16.899296Z","shell.execute_reply.started":"2025-02-07T20:10:16.880730Z","shell.execute_reply":"2025-02-07T20:10:16.898465Z"},"editable":false},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"First Try With 20 Activities and 3 Resources","metadata":{"editable":false}},{"cell_type":"code","source":"if __name__ == '__main__':\n    num_episodes = 15\n    state_dim = 5  # Each node: [duration, resource1, resource2, resource3, is_scheduled]\n    num_tasks = 20\n    resource_limits = [10, 10, 10]\n    action_dim = num_tasks\n\n    # DQN Agent\n    print(\"=== Training DQN Agent ===\")\n    env_dqn = RCPSPEnv(num_tasks=num_tasks, resource_limits=resource_limits)\n    dqn_agent = DQNAgent(state_dim, action_dim)\n    dqn_rewards, dqn_makespans, best_dqn_policy, avg_deviation_dqn = run_agent(dqn_agent, env_dqn, num_episodes, agent_name='DQN')\n\n    # DDQN Agent\n    print(\"\\n=== Training DDQN Agent ===\")\n    env_ddqn = RCPSPEnv(num_tasks=num_tasks, resource_limits=resource_limits)\n    ddqn_agent = DDQNAgent(state_dim, action_dim)\n    ddqn_rewards, ddqn_makespans, best_ddqn_policy, avg_deviation_ddqn = run_agent(ddqn_agent, env_ddqn, num_episodes, agent_name='DDQN')\n\n    # PPO Agent\n    print(\"\\n=== Training PPO Agent ===\")\n    env_ppo = RCPSPEnv(num_tasks=num_tasks, resource_limits=resource_limits)\n    ppo_agent = PPOAgent(state_dim, action_dim)\n    ppo_rewards, ppo_makespans, best_ppo_policy, avg_deviation_ppo = run_agent(ppo_agent, env_ppo, num_episodes, agent_name='PPO')\n\n    print(\"\\n=== RL Agents Summary ===\")\n    print(f\"DQN Average Makespan: {np.mean(dqn_makespans):.2f}, Average Deviation: {avg_deviation_dqn:.2f}\")\n    print(f\"DDQN Average Makespan: {np.mean(ddqn_makespans):.2f}, Average Deviation: {avg_deviation_ddqn:.2f}\")\n    print(f\"PPO Average Makespan: {np.mean(ppo_makespans):.2f}, Average Deviation: {avg_deviation_ppo:.2f}\")\n\n    # Genetic Programming Hyper-Heuristic Optimization\n    print(\"\\n=== GPHH Optimization ===\")\n    env_gphh = RCPSPEnv(num_tasks=num_tasks, resource_limits=resource_limits)\n    best_weights, best_makespan = env_gphh.optimize_schedule_gphh(population_size=50, generations=100, mutation_prob=0.3)\n    cp_length = env_gphh.compute_critical_path_length()\n    deviation_gphh = best_makespan - cp_length\n    print(f\"Best makespan from GPHH: {best_makespan}\")\n    print(f\"Best weight vector: {best_weights}\")\n    print(f\"Deviation from critical path ({cp_length}): {deviation_gphh}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:10:16.901134Z","iopub.execute_input":"2025-02-07T20:10:16.901381Z","iopub.status.idle":"2025-02-07T20:31:13.036155Z","shell.execute_reply.started":"2025-02-07T20:10:16.901353Z","shell.execute_reply":"2025-02-07T20:31:13.035291Z"},"editable":false},"outputs":[{"name":"stdout","text":"=== Training DQN Agent ===\nDQN Episode 1/15 - Makespan: 82, Deviation: 38, Total Reward: -292, Epsilon: 1.0000\nDQN Episode 2/15 - Makespan: 84, Deviation: 40, Total Reward: -464, Epsilon: 0.9950\nDQN Episode 3/15 - Makespan: 78, Deviation: 34, Total Reward: -538, Epsilon: 0.9900\nDQN Episode 4/15 - Makespan: 86, Deviation: 42, Total Reward: -1386, Epsilon: 0.9851\nDQN Episode 5/15 - Makespan: 83, Deviation: 39, Total Reward: -463, Epsilon: 0.9801\nDQN Episode 6/15 - Makespan: 74, Deviation: 30, Total Reward: -594, Epsilon: 0.9752\nDQN Episode 7/15 - Makespan: 79, Deviation: 35, Total Reward: -519, Epsilon: 0.9704\nDQN Episode 8/15 - Makespan: 80, Deviation: 36, Total Reward: -350, Epsilon: 0.9655\nDQN Episode 9/15 - Makespan: 80, Deviation: 36, Total Reward: -1560, Epsilon: 0.9607\nDQN Episode 10/15 - Makespan: 76, Deviation: 32, Total Reward: -466, Epsilon: 0.9559\nDQN Episode 11/15 - Makespan: 89, Deviation: 45, Total Reward: -779, Epsilon: 0.9511\nDQN Episode 12/15 - Makespan: 76, Deviation: 32, Total Reward: -776, Epsilon: 0.9464\nDQN Episode 13/15 - Makespan: 76, Deviation: 32, Total Reward: -506, Epsilon: 0.9416\nDQN Episode 14/15 - Makespan: 78, Deviation: 34, Total Reward: -558, Epsilon: 0.9369\nDQN Episode 15/15 - Makespan: 83, Deviation: 39, Total Reward: -953, Epsilon: 0.9322\nBest DQN policy found in Episode 6 with Makespan: 74\nAverage deviation from critical path (44): 36.266666666666666\n\n=== Training DDQN Agent ===\nDDQN Episode 1/15 - Makespan: 90, Deviation: 25, Total Reward: -630, Epsilon: 1.0000\nDDQN Episode 2/15 - Makespan: 90, Deviation: 25, Total Reward: -1100, Epsilon: 0.9950\nDDQN Episode 3/15 - Makespan: 105, Deviation: 40, Total Reward: -1025, Epsilon: 0.9900\nDDQN Episode 4/15 - Makespan: 92, Deviation: 27, Total Reward: -1552, Epsilon: 0.9851\nDDQN Episode 5/15 - Makespan: 87, Deviation: 22, Total Reward: -637, Epsilon: 0.9801\nDDQN Episode 6/15 - Makespan: 83, Deviation: 18, Total Reward: -663, Epsilon: 0.9752\nDDQN Episode 7/15 - Makespan: 89, Deviation: 24, Total Reward: -349, Epsilon: 0.9704\nDDQN Episode 8/15 - Makespan: 86, Deviation: 21, Total Reward: -576, Epsilon: 0.9655\nDDQN Episode 9/15 - Makespan: 91, Deviation: 26, Total Reward: -381, Epsilon: 0.9607\nDDQN Episode 10/15 - Makespan: 95, Deviation: 30, Total Reward: -585, Epsilon: 0.9559\nDDQN Episode 11/15 - Makespan: 81, Deviation: 16, Total Reward: -701, Epsilon: 0.9511\nDDQN Episode 12/15 - Makespan: 95, Deviation: 30, Total Reward: -885, Epsilon: 0.9464\nDDQN Episode 13/15 - Makespan: 84, Deviation: 19, Total Reward: -304, Epsilon: 0.9416\nDDQN Episode 14/15 - Makespan: 90, Deviation: 25, Total Reward: -440, Epsilon: 0.9369\nDDQN Episode 15/15 - Makespan: 98, Deviation: 33, Total Reward: -558, Epsilon: 0.9322\nBest DDQN policy found in Episode 11 with Makespan: 81\nAverage deviation from critical path (65): 25.4\n\n=== Training PPO Agent ===\nPPO Episode 1/15 - Makespan: 90, Deviation: 41, Total Reward: -550, Epsilon: N/A\nPPO Episode 2/15 - Makespan: 75, Deviation: 26, Total Reward: -1805, Epsilon: N/A\nPPO Episode 3/15 - Makespan: 80, Deviation: 31, Total Reward: -1670, Epsilon: N/A\nPPO Episode 4/15 - Makespan: 82, Deviation: 33, Total Reward: -3802, Epsilon: N/A\nPPO Episode 5/15 - Makespan: 83, Deviation: 34, Total Reward: -4163, Epsilon: N/A\nPPO Episode 6/15 - Makespan: 102, Deviation: 53, Total Reward: -1672, Epsilon: N/A\nPPO Episode 7/15 - Makespan: 91, Deviation: 42, Total Reward: -2211, Epsilon: N/A\nPPO Episode 8/15 - Makespan: 87, Deviation: 38, Total Reward: -5477, Epsilon: N/A\nPPO Episode 9/15 - Makespan: 91, Deviation: 42, Total Reward: -2511, Epsilon: N/A\nPPO Episode 10/15 - Makespan: 83, Deviation: 34, Total Reward: -4133, Epsilon: N/A\nPPO Episode 11/15 - Makespan: 95, Deviation: 46, Total Reward: -2285, Epsilon: N/A\nPPO Episode 12/15 - Makespan: 90, Deviation: 41, Total Reward: -16580, Epsilon: N/A\nPPO Episode 13/15 - Makespan: 102, Deviation: 53, Total Reward: -6772, Epsilon: N/A\nPPO Episode 14/15 - Makespan: 90, Deviation: 41, Total Reward: -14840, Epsilon: N/A\nPPO Episode 15/15 - Makespan: 83, Deviation: 34, Total Reward: -14683, Epsilon: N/A\nBest PPO policy found in Episode 2 with Makespan: 75\nAverage deviation from critical path (49): 39.266666666666666\n\n=== RL Agents Summary ===\nDQN Average Makespan: 80.27, Average Deviation: 36.27\nDDQN Average Makespan: 90.40, Average Deviation: 25.40\nPPO Average Makespan: 88.27, Average Deviation: 39.27\n\n=== GPHH Optimization ===\nBest makespan from GPHH: 71\nBest weight vector: [3.046947332801946, -0.22511197082801498, 9.644214869105442, 8.47454904975664]\nDeviation from critical path (53): 18\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Second Try With 30 Activities and 4 Resources","metadata":{"editable":false}},{"cell_type":"code","source":"if __name__ == '__main__':\n    num_episodes = 15\n    state_dim = 6  # Each node: [duration, resource1, resource2, resource3, is_scheduled]\n    num_tasks = 30\n    resource_limits = [12, 6, 10, 8]\n    action_dim = num_tasks\n\n    # DQN Agent\n    print(\"=== Training DQN Agent ===\")\n    env_dqn = RCPSPEnv(num_tasks=num_tasks, resource_limits=resource_limits)\n    dqn_agent = DQNAgent(state_dim, action_dim)\n    dqn_rewards, dqn_makespans, best_dqn_policy, avg_deviation_dqn = run_agent(dqn_agent, env_dqn, num_episodes, agent_name='DQN')\n\n    # DDQN Agent\n    print(\"\\n=== Training DDQN Agent ===\")\n    env_ddqn = RCPSPEnv(num_tasks=num_tasks, resource_limits=resource_limits)\n    ddqn_agent = DDQNAgent(state_dim, action_dim)\n    ddqn_rewards, ddqn_makespans, best_ddqn_policy, avg_deviation_ddqn = run_agent(ddqn_agent, env_ddqn, num_episodes, agent_name='DDQN')\n\n    # PPO Agent\n    print(\"\\n=== Training PPO Agent ===\")\n    env_ppo = RCPSPEnv(num_tasks=num_tasks, resource_limits=resource_limits)\n    ppo_agent = PPOAgent(state_dim, action_dim)\n    ppo_rewards, ppo_makespans, best_ppo_policy, avg_deviation_ppo = run_agent(ppo_agent, env_ppo, num_episodes, agent_name='PPO')\n\n    print(\"\\n=== RL Agents Summary ===\")\n    print(f\"DQN Average Makespan: {np.mean(dqn_makespans):.2f}, Average Deviation: {avg_deviation_dqn:.2f}\")\n    print(f\"DDQN Average Makespan: {np.mean(ddqn_makespans):.2f}, Average Deviation: {avg_deviation_ddqn:.2f}\")\n    print(f\"PPO Average Makespan: {np.mean(ppo_makespans):.2f}, Average Deviation: {avg_deviation_ppo:.2f}\")\n\n    # Genetic Programming Hyper-Heuristic Optimization\n    print(\"\\n=== GPHH Optimization ===\")\n    env_gphh = RCPSPEnv(num_tasks=num_tasks, resource_limits=resource_limits)\n    best_weights, best_makespan = env_gphh.optimize_schedule_gphh(population_size=50, generations=100, mutation_prob=0.3)\n    cp_length = env_gphh.compute_critical_path_length()\n    deviation_gphh = best_makespan - cp_length\n    print(f\"Best makespan from GPHH: {best_makespan}\")\n    print(f\"Best weight vector: {best_weights}\")\n    print(f\"Deviation from critical path ({cp_length}): {deviation_gphh}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:32:22.530464Z","iopub.execute_input":"2025-02-07T20:32:22.530823Z","execution_failed":"2025-02-08T04:37:57.821Z"},"editable":false},"outputs":[{"name":"stdout","text":"=== Training DQN Agent ===\nDQN Episode 1/15 - Makespan: 163, Deviation: 98, Total Reward: -1063, Epsilon: 1.0000\nDQN Episode 2/15 - Makespan: 166, Deviation: 101, Total Reward: -806, Epsilon: 0.9950\nDQN Episode 3/15 - Makespan: 164, Deviation: 99, Total Reward: -894, Epsilon: 0.9900\nDQN Episode 4/15 - Makespan: 163, Deviation: 98, Total Reward: -1383, Epsilon: 0.9851\nDQN Episode 5/15 - Makespan: 175, Deviation: 110, Total Reward: -795, Epsilon: 0.9801\nDQN Episode 6/15 - Makespan: 162, Deviation: 97, Total Reward: -1742, Epsilon: 0.9752\nDQN Episode 7/15 - Makespan: 168, Deviation: 103, Total Reward: -978, Epsilon: 0.9704\nDQN Episode 8/15 - Makespan: 170, Deviation: 105, Total Reward: -1130, Epsilon: 0.9655\nDQN Episode 9/15 - Makespan: 176, Deviation: 111, Total Reward: -1006, Epsilon: 0.9607\nDQN Episode 10/15 - Makespan: 171, Deviation: 106, Total Reward: -861, Epsilon: 0.9559\nDQN Episode 11/15 - Makespan: 165, Deviation: 100, Total Reward: -1055, Epsilon: 0.9511\nDQN Episode 12/15 - Makespan: 167, Deviation: 102, Total Reward: -907, Epsilon: 0.9464\nDQN Episode 13/15 - Makespan: 162, Deviation: 97, Total Reward: -1372, Epsilon: 0.9416\nDQN Episode 14/15 - Makespan: 159, Deviation: 94, Total Reward: -969, Epsilon: 0.9369\nDQN Episode 15/15 - Makespan: 164, Deviation: 99, Total Reward: -824, Epsilon: 0.9322\nBest DQN policy found in Episode 14 with Makespan: 159\nAverage deviation from critical path (65): 101.33333333333333\n\n=== Training DDQN Agent ===\nDDQN Episode 1/15 - Makespan: 156, Deviation: 110, Total Reward: -796, Epsilon: 1.0000\nDDQN Episode 2/15 - Makespan: 162, Deviation: 116, Total Reward: -1422, Epsilon: 0.9950\nDDQN Episode 3/15 - Makespan: 174, Deviation: 128, Total Reward: -974, Epsilon: 0.9900\nDDQN Episode 4/15 - Makespan: 165, Deviation: 119, Total Reward: -1525, Epsilon: 0.9851\nDDQN Episode 5/15 - Makespan: 167, Deviation: 121, Total Reward: -647, Epsilon: 0.9801\nDDQN Episode 6/15 - Makespan: 184, Deviation: 138, Total Reward: -814, Epsilon: 0.9752\nDDQN Episode 7/15 - Makespan: 166, Deviation: 120, Total Reward: -606, Epsilon: 0.9704\nDDQN Episode 8/15 - Makespan: 164, Deviation: 118, Total Reward: -2054, Epsilon: 0.9655\nDDQN Episode 9/15 - Makespan: 180, Deviation: 134, Total Reward: -570, Epsilon: 0.9607\nDDQN Episode 10/15 - Makespan: 187, Deviation: 141, Total Reward: -937, Epsilon: 0.9559\nDDQN Episode 11/15 - Makespan: 179, Deviation: 133, Total Reward: -1089, Epsilon: 0.9511\nDDQN Episode 12/15 - Makespan: 175, Deviation: 129, Total Reward: -1365, Epsilon: 0.9464\nDDQN Episode 13/15 - Makespan: 168, Deviation: 122, Total Reward: -758, Epsilon: 0.9416\nDDQN Episode 14/15 - Makespan: 175, Deviation: 129, Total Reward: -695, Epsilon: 0.9369\nDDQN Episode 15/15 - Makespan: 169, Deviation: 123, Total Reward: -1109, Epsilon: 0.9322\nBest DDQN policy found in Episode 1 with Makespan: 156\nAverage deviation from critical path (46): 125.4\n\n=== Training PPO Agent ===\nPPO Episode 1/15 - Makespan: 225, Deviation: 175, Total Reward: -1425, Epsilon: N/A\nPPO Episode 2/15 - Makespan: 246, Deviation: 196, Total Reward: -3266, Epsilon: N/A\nPPO Episode 3/15 - Makespan: 227, Deviation: 177, Total Reward: -6777, Epsilon: N/A\nPPO Episode 4/15 - Makespan: 222, Deviation: 172, Total Reward: -17622, Epsilon: N/A\nPPO Episode 5/15 - Makespan: 224, Deviation: 174, Total Reward: -7564, Epsilon: N/A\nPPO Episode 6/15 - Makespan: 221, Deviation: 171, Total Reward: -11881, Epsilon: N/A\nPPO Episode 7/15 - Makespan: 227, Deviation: 177, Total Reward: -67597, Epsilon: N/A\nPPO Episode 8/15 - Makespan: 233, Deviation: 183, Total Reward: -6133, Epsilon: N/A\nPPO Episode 9/15 - Makespan: 230, Deviation: 180, Total Reward: -17720, Epsilon: N/A\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}